---
title: "Project_vesion6"
author: "Group 20: Iman, Mostafa, Olusegun, Virtus"
date: 11/30/2021
output: 
    html_notebook:
        toc: yes
---

---

# Meet Data: "Hitters"
```{r}
rm(list = ls())
library(ISLR)
df1 = Hitters
df1 = df1[complete.cases(df1),]

Table = data.frame(table(df1$League), table(df1$Division), table(df1$NewLeague))
names(Table)[1] = "League"
names(Table)[3] = "Division"
names(Table)[5] = "NewLeague"
#View(Table)
Table
str(df1)
```
- We have 16 continuous variables
- Also, we have 3 categorical variables.

## Full MLR model
```{r}
mlr1 = lm(Salary ~ . , data = df1)
summary(mlr1)
```


### Check linearity assumptions     
#### scatter plots for response variable vs. the regressors
```{r}
#pairs(df1)
# pairs(df1[ , c(17, 1:4)])
# pairs(df1[ , c(17, 5:8)])
# pairs(df1[ , c(17, 9:12)])
# pairs(df1[ , c(17, 13:16)])

# 
par(mfcol = c(2, 4))
plot(df1$AtBat, df1$Salary,
     xlab = "AtBat",
     ylab = "Salary")
plot(df1$Hits, df1$Salary,
     xlab = "Hits",
     ylab = "Salary")
plot(df1$HmRun, df1$Salary,
     xlab = "HmRun",
     ylab = "Salary")
plot(df1$Runs, df1$Salary,
     xlab = "Runs",
     ylab = "Salary")
plot(df1$RBI, df1$Salary,
     xlab = "RBI",
     ylab = "Salary")
plot(df1$Walks, df1$Salary,
     xlab = "Walks",
     ylab = "Salary")
plot(df1$Years, df1$Salary,
     xlab = "Years",
     ylab = "Salary")
plot(df1$CAtBat, df1$Salary,
     xlab = "CAtBat",
     ylab = "Salary")
plot(df1$CHits, df1$Salary,
     xlab = "CHits",
     ylab = "Salary")
plot(df1$CHmRun, df1$Salary,
     xlab = "CHmRun",
     ylab = "Salary")
plot(df1$CRuns, df1$Salary,
     xlab = "CRuns",
     ylab = "Salary")
plot(df1$CRBI, df1$Salary,
     xlab = "CRBI",
     ylab = "Salary")
plot(df1$CWalks, df1$Salary,
     xlab = "CWalks",
     ylab = "Salary")
plot(df1$PutOuts, df1$Salary,
     xlab = "PutOuts",
     ylab = "Salary")
plot(df1$Assists, df1$Salary,
     xlab = "Assists",
     ylab = "Salary")
plot(df1$Errors, df1$Salary,
     xlab = "Errors",
     ylab = "Salary")

```
#### interpretation 
Variables like Years, Runs, Walks, and Assists do not have a linear relationship with the response (Salary)

#### Checking for zero mean and constant variance of error terms       
Residual errors vs. the Fitted values       
```{r}
plot(mlr1$fitted.values, mlr1$residuals,
     xlab = "Fitted value",
     ylab = "Residuals",
     main = "Check for 0 mean and constant var \n  Residual vs. Fitted value")
list1 = seq(0,2000,200)
abline(h=0, v = list1, col="red")
```
#### Comments    
- If we imagine some vertical strips we realize that the average mean is not zero     
- If we scan the plot from left to right that variance is not constant.   

#### Check for assumption of independence of error terms
```{r}
#row_num <- c(1:nrow(Dat))
# sort_x1 <- sort(Dat$hp, index.return=TRUE)
# plot(row_num, mlr1$residuals[sort_x1$ix],
# main = "Check for independence \n Residuals sorted by hp")
# abline(h=0)
```



#### Normal Probability Plot       
```{r}
qqnorm(mlr1$residuals)
qqline(mlr1$residuals)
```
#### Comments    
the dataset is not normal, there are some point that far away from from the straight line.   

#### Perform Shapiro-Wilk test for normality     
```{r}
shapiro.test(mlr1$residuals)
```
#### Comments    
According to the Shapiro-Wilk test, the p-value is 6.602e-10 which is less than 0.05. So we reject the null hypothesis and conclude that the dataset does not follow a normal distribution.     

### VIF   
```{r}
library(car)
vif(mlr1)
```
#### Comments    
The model suffers from high multicollinearity. there are some variables that their VIF value is more than 250. As we know the threshold for VIF is 4.


### Pearson Correlation    
```{r}
library(corrplot)
df_1 = df1[ , -c(14,15,20)]
M = cor(df_1)
#corrplot.mixed(M, order = 'AOE')
corrplot(M, order = 'AOE')
```
#### Comments    
- ATBat has a strong correlation with Hits, Runs, RBI, and Walk.   
- there is a significant correlation between RBI and AtBat,Hits, Run and HmRun.  
- Variables like CRBI, CRuns, Chits, CAtBat, CWalk, and years have a strong correlation together.

##  Variable Selection    
### Anova Type II
```{r}
Anova(mlr1, type = 2)
```
### Droping Some Variables manually    
We drop variables with the lowest contribution in the model   
```{r}
rm(list = ls())
df1 = Hitters
df1 = df1[complete.cases(df1),]
df2 = df1 [ , -c(3,4,5,7,9,10,17,18,20)]
mlr2 = lm(Salary ~ . , data = df2)
summary(mlr2)
```
The adjusted r-square increased from 0.5106 to 0.519 but did not change significantly  

## Testing whether the present of categorical variables have significant impact on Salary or not
```{r}
linearHypothesis(mlr2, c("DivisionW = 0"))
linearHypothesis(mlr2, c("LeagueN  = 0"))
```
The p_value for both categorical variables is more than 0.05. We conclude that these variables do not have a significant impact on the response (Salary)   

## Transformation    
### Using box-cox   
```{r}
library(MASS)
boxcox(mlr2)
mlr3 <- lm((Salary)^(0.25) ~ ., data = df2)
summary(mlr3)

```




### Transformation   
We used power transformation p=0.25 for response and log transformation for `CRBI` variable
```{r}
mlr4 = lm(Salary^0.25 ~ AtBat + Hits + Walks + CAtBat + CRuns + log(CRBI) + 
    CWalks + League + Division + PutOuts   , data = df2)
summary(mlr4)
```
The value of adjusted R-square increased significantly to 0.5928. 


## removing influential points   
```{r}
k= ncol(df2)-1
n= nrow(df2)

which(abs(dffits(mlr4)) > 2*sqrt((k+2)/(n-k-2))) #influential if abs(dffits) > 2*sqrt((k+2)/(n-k-2))
which(cooks.distance(mlr4) > 1) #influential if Cook's distance > 1
which(covratio(mlr4) > (1 + 3*(k+1)/n)) #influential if covratio > 1 + 3*(k+1)/n OR covratio < 1 - 3*(k+1)/n
which(covratio(mlr4) < (1 - 3*(k+1)/n))

### Finding common influential points      
intersect(which(abs(dffits(mlr4)) > 2*sqrt((k+2)/(n-k-2))), which(covratio(mlr4) > (1 + 3*(k+1)/n)))
intersect(which(abs(dffits(mlr4)) > 2*sqrt((k+2)/(n-k-2))), which(covratio(mlr4) < (1 - 3*(k+1)/n)))
```


#### Removing Influential Points      
```{r}
df3 = df2[ -c(21,55,120,173,183,189,230,241), ]

mlr5 = lm(Salary^0.25 ~ AtBat + Hits + Walks + CAtBat + CRuns + log(CRBI) + 
    CWalks + League + Division + PutOuts   , data = df3)
summary(mlr5)
```

### Adjusted R-Square    
For the final model the value of adjusted R-square increased significantly to 0.7528.

### Linearity Assumptions     
#### Checking for zero mean and constant variance of error terms        
```{r}
plot(mlr5$fitted.values, mlr5$residuals,
     xlab = "Fitted value",
     ylab = "Residuals",
     main = "Check for 0 mean and constant var \n  Residual vs. Fitted value")
list1 = seq(2,7,1)
abline(h=0, v = list1, col="red")
```

#### Interpretation    
- If we compare the new plot with previous one we can realize that there is an improvement. Except the firs part of plot, we can see that the points are distributed more symmetrically than before.


#### Normal Probability Plot       
```{r}
qqnorm(mlr5$residuals)
qqline(mlr5$residuals)

```
#### Interpretation    
As we can see most of the points fall on the straight line that indicates data distributed normaly. 

#### ShapiroWilk test     
```{r}
shapiro.test(mlr5$residuals)

```
#### Interpretation    
p-value = 0.7332 ==> we fail to reject the null ==> The errors foloow a normal distribution.    

### Final Model    
#### Equation of fitted model     
$\widehat{Salary^{0.25}} = 0.4513324 - 0.0022757*AtBat + 0.0086190*Hits +0.0071524*Walks - 0.0003134*CAtBat$   
$+ 0.0027842*CRuns + 0.7165028*log(CRBI) - 0.0012647* CWalks + 0.1309695*LeagueN - 0.0893950*DivisionW + 0.0003267*PutOuts$

#### ANOVA II  
```{r}
Anova(mlr5, type = 2)
```


### Predicting a Salary  
```{r}
#summary(df4)

# A random point (Midian points) # Median point for Salary = 416
x0 = data.frame(AtBat=413, Hits=103, Walks = 37, CAtBat=1928 , CRuns=274 , CRBI=226 , CWalks=172 , League= "A", Division= "W",  PutOuts=222)
(predict(mlr5, x0, interval = "prediction", level = 0.95))^4

# predicting Rance Mulliniks Salary # real value is 450
#                  AtBat Hits Walks CAtBat CRuns CRBI CWalks League Division PutOuts Salary
# Rance Mulliniks   348   90    43   2288   295  273    269      A        E      60    450
x1 = data.frame(AtBat=348, Hits=90, Walks = 43, CAtBat=2288 , CRuns=295 , CRBI=273 , CWalks=269 , League= "A", Division= "E",  PutOuts=450)
#(predict(mlr5, x1, interval = "prediction", level = 0.95))^4
(predict(mlr5, x1))^4


```
- For median point the predicted Salary is 400 which is so closed to real value, 416.     
- Also, the predicted value for Rance Mulliniks, one of the observation, is 476 that is so closed to real value, 450 
- We can conclude that the final model can predict Salary with a high precision.    






#### Check significance of categorical variables    
```{r}
linearHypothesis(mlr5, c("DivisionW = 0"))
linearHypothesis(mlr5, c("LeagueN  = 0"))
```

